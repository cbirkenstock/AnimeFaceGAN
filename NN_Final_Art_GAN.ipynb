{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJEQk2K3sa7"
      },
      "source": [
        "# [Blog](https://docs.google.com/document/d/1K4prKKw67B_64ljlggcfVN29FOKcMnRuofFIp2XLuwI/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415pytUi83iU"
      },
      "source": [
        "## Anime GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The imports here are pretty standard. They include: \n",
        "- numpy (basic array manipulation, opening npy files, etc)\n",
        "- matplotlib (creating graphs)\n",
        "- os (file directory)\n",
        "- tensorflow (machine learning framework)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDoZIGAstHSP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a python script we used to locally download the anime dataset to our laptops locally. One we had the images we re-shaped then into arrays so they were properly formatted for our neural network. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "_HPyowCtA87l",
        "outputId": "24051f8f-eec2-4016-e0c6-7f0f1037dc0f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 100\n",
        "IMAGE_CHANNELS = 3\n",
        "IMAGE_DIR = os.path.expanduser(\"~/Desktop/AnimeImages/\")\n",
        "NUM_IMAGES = 60000\n",
        "\n",
        "\n",
        "onlyFiles = [f for f in listdir(IMAGE_DIR) if isfile(join(IMAGE_DIR, f))]\n",
        "\n",
        "training_data = []\n",
        "\n",
        "for file in onlyFiles:\n",
        "    path = os.path.join(IMAGE_DIR, file)\n",
        "    try:\n",
        "        image = Image.open(path).resize(\n",
        "            (IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
        "    except:\n",
        "        continue\n",
        "    training_data.append(np.asarray(image))\n",
        "\n",
        "training_data = np.reshape(\n",
        "    training_data, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "training_data = training_data / 127.5 - 1\n",
        "\n",
        "np.save('anime_data.npy', training_data[:NUM_IMAGES])\n",
        "\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Back in the cloud here, we manually upload are anime_data.npy file to either Google Colab or Kaggle and load it into our variable training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr1bl6KTODYx",
        "outputId": "6b570787-82a5-4f6c-aa26-4b14dd742615"
      },
      "outputs": [],
      "source": [
        "# Load the training data\n",
        "if os.path.isfile('anime_data.npy'):\n",
        "  training_data = np.load('anime_data.npy')\n",
        "  print('Training data loaded successfully.')\n",
        "  print(f'Shape of data: {training_data.shape}')\n",
        "else:\n",
        "  raise Exception('You need to import the anime_data.npy file into the runtime.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the structure of our generator and Discriminator. Both our generator and discriminator have three dense layers of 128 nodes each and use leakyReLu activation functions. At the Bottom of the cell we create the GAN by adding both the generator and discriminator to the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-JzoJg2lYAh"
      },
      "outputs": [],
      "source": [
        "def build_generator(img_shape, z_dim):\n",
        "    model = Sequential()\n",
        "    #The first layer is taking in 100 dimensional noise vector that follows a Gaussian Curve\n",
        "    model.add(Dense(128, input_dim=z_dim))\n",
        "    #All the nodes in our generator have a LeakyReLu activation function. Relu in general is prefered to sigmoid because is \n",
        "    #much faster to compute as is its derivative. However, the regular ReLu funciton can cause nodes to irreversably \"die\"\n",
        "    #due to negiatve numbers all becoming zero. LeakyReLu solves that problem by just making negative numbers very small, not zero. \n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(128, input_dim=z_dim))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(128, input_dim=z_dim))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    #last output will be the size of a 100x100x3 image becuase it is generating the image\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh')) \n",
        "    #finally, model shapes it's output from a flat 100x100x3 dimensional vector into the shape an an image\n",
        "    model.add(Reshape(img_shape))\n",
        "    return model\n",
        "\n",
        "def build_discriminator(img_shape):\n",
        "    model = Sequential()\n",
        "    #For input, this model is receiving nested arrays in the shape of 100x100x3 images and this needs to be simplified to a single array \n",
        "    #with all the numbers in one vector -- this is what Flatten does\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    #The output is only one neuron becuase it is just guessing on whether the anime face is real or not\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "    \n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we build actual instances of the generator, discriminator, and GAN. The variables at the top describe how large the images will be, how many layers of an image there will be (here three for RGB) and how many dimensions our noise vector will have. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wCdvn4gHDU8",
        "outputId": "a315797c-1323-430c-e3e7-29589ff37a7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_size = 100\n",
        "img_channels = 3\n",
        "img_shape = (img_size, img_size, img_channels)\n",
        "z_dim = 100\n",
        "\n",
        "# Create discriminator\n",
        "discriminator = build_discriminator(img_shape)\n",
        "#the loss funciton here (which is used to determine how to evaluate the models performance) is binary cross-entropy since the \n",
        "#discriminator is making a binary decision -- is the image that it received real or fake?\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Create generator\n",
        "generator = build_generator(img_shape, z_dim)\n",
        "\n",
        "# Create GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "#Why binary_crossentropy is used here is less intuitive, since the generator creates a 100 dimensional vector as an output \n",
        "#instead of a single number. However, if you think about it, it makes sense. The generator is not judged on the the image it \n",
        "#makes in a vacuum. It's performance is also based on the binary decision of the discriminator (i.e., did it fool it or not), \n",
        "#ehich is also a binary decision. \n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After every x number of training iterations for the GAN, sample_images is called to have the generator make predictions at it's current state. These pictures are then put onto a plot so we cna visualize the GANS improvement over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_images(generator, image_grid_rows=4, image_grid_columns=4, seed=False):\n",
        "  # Sample a number of random images from the generator and plot them in a grid\n",
        "  if seed:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "  z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "  gen_imgs = generator.predict(z)\n",
        "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "  fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4, 4), sharey=True, sharex=True)\n",
        "\n",
        "  cnt = 0\n",
        "  for i in range(image_grid_rows):\n",
        "    for j in range(image_grid_columns):\n",
        "      axs[i, j].imshow(gen_imgs[cnt, :, :])\n",
        "      axs[i, j].axis('off')\n",
        "      cnt += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the most complex part of the process -- actually training the GAN. The function essientially is a for-loop that does n iterations. For each iteration, a random sample of the real images is taken, and an equal number of fake images are generated. Then the discriminator is tested to see how well it can acurrately discriminate on both the real and fake images separately, and the generator is also evaluated on how well it creates fake images based on how often it fools the discriminator. Both models are then changed accordingly. Every n iterations, a sample plot is made with the helper function above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZabVokZla8N"
      },
      "outputs": [],
      "source": [
        "def train(iterations, batch_size, sample_interval):\n",
        "  storeloss = []\n",
        "  accuracies = []\n",
        "  iteration_checkpoints = []\n",
        "\n",
        "  X_train = training_data\n",
        "\n",
        "  real = np.ones((batch_size, 1))\n",
        "  fake = np.zeros((batch_size, 1))\n",
        "\n",
        "  img_num = 0\n",
        "\n",
        "  # Training loop\n",
        "  for iteration in range(iterations):\n",
        "    # This gets batch_size amount of numbers between zero and the amount of images in X_train - 1. Images from those inexes\n",
        "    # are then taken from X_train as a random sample\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    #This creates an equal amount of 100 dimensional noise vectors for the generator so there are an equal amount of real and\n",
        "    #fake images\n",
        "    z = np.random.normal(0, 1, (batch_size, 100))\n",
        "    gen_imgs = generator.predict(z)\n",
        "\n",
        "    # Here the discriminator tests on real and fake data seperately and gets accuracy scores for both. Its overall acurracy \n",
        "    # is the average of the two. the variable \"real\" is an array of ones that the discriminator is evaluated against when \n",
        "    # judging real images (as the correct answer should always be one) and the same is true for \"fake\" except it's an array \n",
        "    # of zeroes. \n",
        "    d_loss_acc_real = discriminator.train_on_batch(imgs, real)\n",
        "    d_loss_acc_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "    d_loss, accuracy = 0.5 * np.add(d_loss_acc_real, d_loss_acc_fake)\n",
        "\n",
        "    #Here a new batch of random noise generators are created and how well the generator does is calculated.\n",
        "    z = np.random.normal(0, 1, (batch_size, 100))\n",
        "    gen_imgs = generator.predict(z)\n",
        "    g_loss = gan.train_on_batch(z, real)\n",
        "\n",
        "    #if it's a certain iteration the stats will be printed an sample_images will be called\n",
        "    if iteration % sample_interval == 0:\n",
        "\n",
        "      storeloss.append((d_loss, g_loss))\n",
        "      accuracies.append(100.0 * accuracy)\n",
        "      iteration_checkpoints.append(iteration)\n",
        "\n",
        "      print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % \n",
        "            (iteration, d_loss, 100.0 * accuracy, g_loss))\n",
        "\n",
        "      # Visualize the performance of the generator by producing images from the test vector\n",
        "      plt.figure(figsize=(10,10))\n",
        "      sample_images(generator, seed=42)\n",
        "      plt.axis('off')\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f'iteration_images/{img_num}.png')\n",
        "      img_num +=1\n",
        "      plt.close('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And Finally, we can train our data, changing the number of iterations, batch size, and sample iterval as we please. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nSfSKBJjs3K2",
        "outputId": "9e76c5f2-d9d4-48fc-a689-0d04d4e73f70"
      },
      "outputs": [],
      "source": [
        "!rm -r 'iteration_images/'\n",
        "!mkdir 'iteration_images/'\n",
        "\n",
        "iterations = 20000\n",
        "batch_size = 128\n",
        "sample_interval = 200\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Rff19j8-H0"
      },
      "source": [
        "## GIF Creator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have completed all of our training iterations (whew), we can take the grids made and create a GIF to show our progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvtNDB6OztDt"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "images = []\n",
        "\n",
        "for filename in os.listdir(\"/content/iteration_images/\"):\n",
        "    # print(filename)\n",
        "    images.append(imageio.imread(\"/content/iteration_images/\" + filename))\n",
        "\n",
        "imageio.mimsave('anime_28.gif', images, duration=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSwqYCNe91K3"
      },
      "source": [
        "## Noise Predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, after we have a fully trained generator, we can create an model that does the exact opposite process. Instead of taking in a vector of noise, and creating an image, this model learns to take in an image, and create an appropriate vector of noise. Why would we do this? So that we can feed it an image of a real person, decompose it into noise, and then feed that noise back into the generator to get an anime version of oneself!\n",
        "\n",
        "This network is a convolutional neural network which means that instead of passing the whole image through the network, the image is first reduced down with filters into a smaller grid which represents patters in the picture. This simplified version of the picture is then passed through to train the model. A more techincal explanation can be found in the inline code below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "sO61rXWUMbxk",
        "outputId": "4a7daa48-1de4-431d-c4a3-3a0d72ca072b"
      },
      "outputs": [],
      "source": [
        "def make_noise_predictor(img_size):\n",
        "    model = tf.keras.Sequential()\n",
        "    #This is portion different from the more vanilla neural networks we saw above. Analyzing images pixel by pixel can be very \n",
        "    #computationally expensive and it can also cause models to miss the forest for the trees. What this does next line does is\n",
        "    #create several filters that are 5x5 in dimension and have different patterns. These filters are then run across the image, and create a score\n",
        "    #where points are awarded per pixel if both the image and the filter are lit up and none are avoided if the pixels from the image\n",
        "    #and filter don't match. Then the dot product is calculated and added to a feature map which is a essentially a smaller grid with each pixel \n",
        "    #value representing how each close each portion of the came to matching  a particular pattern. \n",
        "    model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[img_size, img_size, 3]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    #Max Pooling is a continuation of the convolutional process where for each 2x2 grid in the feature map, the max value is taken\n",
        "    #and added to a new, even smaller grid. At this point, the original image has been reduced to a very large array of pixels, to \n",
        "    # a much smaller array which recognizes patters. \n",
        "    \n",
        "    #In the end, this is much easier for the computer to deal with and should intuitively make\n",
        "    #sense even if the specifics aren't fully clear. When humans see a picture of a car, they don't look at each pixel, \n",
        "    #they notice patterns that identify it as a car (windows, doors, antenna, etc.)\n",
        "    model.add(layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='same'))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='same'))\n",
        "    \n",
        "    model.add(layers.Conv2D(96, (5, 5), strides=(1, 1), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='same'))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(100, activation='linear'))    \n",
        "    model.add(layers.Dense(100, activation='linear'))\n",
        "    model.add(layers.Dense(100, activation='linear'))    \n",
        "\n",
        "    return model\n",
        "\n",
        "training_size = 5000\n",
        "img_size = 28\n",
        "epochs = 50\n",
        "\n",
        "Y_train = np.random.normal(0, 1, (training_size, 100))\n",
        "X_train = generator.predict(Y_train)\n",
        "\n",
        "noisePredictor = make_noise_predictor(img_size)\n",
        "noisePredictor.compile(loss='MSE', optimizer=Adam(), metrics=['accuracy'])\n",
        "noisePredictor.fit(X_train, Y_train, batch_size=128, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this creates a single 100 dimensional vector of noise\n",
        "imageNoise = np.random.normal(0, 1, (1, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#here, we create a generated image as usual from the noise, but then we reverse engineer the generated image into the noise\n",
        "#that the noise predictor thinks was used to make it. We then take that prediction and put it back in the generator to see\n",
        "#how different the results are. \n",
        "generatedImage = generator.predict(imageNoise)\n",
        "reverseEngineeredNoise = noisePredictor.predict(generatedImage)\n",
        "reconstructedImage = generator.predict(reverseEngineeredNoise)\n",
        "\n",
        "#for these two blobs we are just loading in actual pictures of faces and formatting them \n",
        "face_1 = np.load('/kaggle/input/realfacepictures/colin_anime_data_64.npy')\n",
        "face_1 = (face_1 - 127.5) / 127.5\n",
        "face_1 = face_1.reshape(1, 64, 64, 3)\n",
        "\n",
        "face_2 = np.load('/kaggle/input/realfacepictures/phil_anime_data_64.npy')\n",
        "face_2 = (face_2 - 127.5) / 127.5\n",
        "face_2 = face_2.reshape(1, 64, 64, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For these four cells below, we are showing the images created above. If the noise predictor is good, generatedImage should look very similar to reconstructedImage. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(generatedImage[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(reconstructedImage[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_1_noise = noisePredictor.predict(face_1)\n",
        "face_1_image = generator.predict(face_1_noise)\n",
        "face_1_image = face_1_image * .5 + .5\n",
        "\n",
        "plt.imshow(face_1_image[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_2_noise = noisePredictor.predict(face_2)\n",
        "face_2_image = generator.predict(face_2_noise)\n",
        "face_2_image = face_2_image * .5 + .5\n",
        "\n",
        "plt.imshow(face_2_image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7ZWvhYE9cUA"
      },
      "source": [
        "## Improved GAN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a different way of creaitng the GAN that used convolutional neural networks. While in theory this should perform better, I did not manipulate it enough to actually produce better results. The iterations were slower and the results were worse so I did not use it in the end. The discriminator works similiary to the noise generator explained above. The one interesting thing here though is that the generater useing convultions but in the reverse direction. Here, it takes smaller bits of information and blows them up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "8Npdr1-3NwRN",
        "outputId": "4fe7624c-6ee9-490d-c41c-a08a08b08d05"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "# BUFFER_SIZE = 5000\n",
        "# BATCH_SIZE = 256\n",
        "\n",
        "train_images = training_data\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 3).astype('float32')\n",
        "print(train_images)\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "\n",
        "\n",
        "def make_generator_model(img_shape, z_dim):\n",
        "    quarterImage = int(img_shape/4)\n",
        "    halfImage = int(img_shape/2)\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(math.pow(quarterImage, 2)*128, use_bias=False, input_shape=(100,)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Reshape((quarterImage, quarterImage, 128)))\n",
        "    assert model.output_shape == (None, quarterImage, quarterImage, 128)  # Note: None is the batch size\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, quarterImage, quarterImage, 64)\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, halfImage, halfImage, 32)\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, img_shape, img_shape, 3)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#64 -> 128\n",
        "def make_discriminator_model(img_shape):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[img_shape, img_shape, 3]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUWRY0rHGwMj",
        "outputId": "be3783bb-a4bb-4da4-b1ec-953fcc3b2b8d"
      },
      "outputs": [],
      "source": [
        "img_size = 28\n",
        "img_channels = 3\n",
        "img_shape = (img_size, img_size, img_channels)\n",
        "z_dim = 100\n",
        "\n",
        "# Create discriminator\n",
        "discriminator = make_discriminator_model(img_size)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(1e-4), metrics=['accuracy'])\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Create generator\n",
        "generator = make_generator_model(img_size, z_dim)\n",
        "generator.summary()\n",
        "\n",
        "# Create GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(1e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GWToBysOGrQZ",
        "outputId": "c710f19a-fbec-4dca-ce6b-bae18986de30"
      },
      "outputs": [],
      "source": [
        "!rm -r 'iteration_images/'\n",
        "!mkdir 'iteration_images/'\n",
        "\n",
        "iterations = 20000\n",
        "batch_size = 128\n",
        "sample_interval = 200\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NN Final: Art GAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
